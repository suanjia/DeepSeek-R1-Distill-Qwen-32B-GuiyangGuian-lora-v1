from transformers import AutoTokenizer, AutoModelForCausalLM
import torch
import streamlit as st
import re

st.set_page_config(layout="wide")

st.markdown(
    """
<style>
.chat-container {
  max-width: 800px;
  margin: auto;
  padding: 1rem;
}
.user-bubble, .assistant-bubble {
  border-radius: 8px;
  padding: 0.75rem 1rem;
  margin: 0.5rem 0;
  line-height: 1.5;
  width: fit-content;
  max-width: 80%;
}
.user-bubble {
  background: #e0e0e0;
  margin-left: auto;
}
.assistant-bubble {
  background: #f0f8ff;
  margin-right: auto;
}
.think-block {
  font-size: 0.9rem;
  color: #555;
  margin-top: 0.25rem;
  background: #fafafa;
  border-left: 3px solid #ccc;
  padding-left: 0.5rem;
}
</style>
""",
    unsafe_allow_html=True
)

# é¡µé¢æ ‡é¢˜
st.markdown("# ğŸ’¬ DeepSeek R1 Distill Chatbot")
st.markdown("ğŸš€ Powered by ç®—å®¶è®¡ç®—")

# æ¨¡å‹å‚æ•°
MODEL_PATH = "deepseek-ai/DeepSeek-R1-Distill-Qwen-32B"

# æ‹†åˆ†â€œæ€è€ƒâ€ä¸â€œå›ç­”â€
def split_text(text):
    m = re.search(r"<think>(.*?)</think>(.*)", text, re.DOTALL)
    if m:
        return m.group(1).strip(), m.group(2).strip()
    return "", text.strip()

# åŠ è½½æ¨¡å‹
@st.cache_resource
def load_model():
    tok = AutoTokenizer.from_pretrained(MODEL_PATH, trust_remote_code=True)
    tok.pad_token = tok.eos_token
    mdl = AutoModelForCausalLM.from_pretrained(
        MODEL_PATH, torch_dtype=torch.bfloat16, device_map="auto"
    )
    return tok, mdl

tokenizer, model = load_model()

# ä¼šè¯å†å²å‚¨å­˜
if "history" not in st.session_state:
    st.session_state.history = [
        {"role": "assistant", "content": "æ‚¨å¥½ï¼Œè¯·é—®æœ‰ä»€ä¹ˆå¯ä»¥å¸®æ‚¨ï¼Ÿ"}
    ]

# èŠå¤©è¾“å…¥æ¡†ï¼ˆæ”¾åœ¨æœ€å‰é¢ä»¥ä¾¿é‡‡é›†æ–°è¾“å…¥ï¼‰
prompt = st.text_input("è¾“å…¥ä½ çš„é—®é¢˜ï¼ŒæŒ‰å›è½¦å‘é€ï¼š", key="input")


if prompt:
    # åŠ å…¥ç”¨æˆ·
    st.session_state.history.append({"role": "user", "content": prompt})

    # æ„é€ æ¨¡å‹è¾“å…¥
    input_text = tokenizer.apply_chat_template(
        st.session_state.history, tokenize=False, add_generation_prompt=True
    )
    inputs = tokenizer([input_text], return_tensors="pt").to(model.device)

    # æ¨¡å‹ç”Ÿæˆï¼ˆä¸é™åˆ¶é•¿åº¦ï¼‰
    max_len = inputs.input_ids.shape[1] + model.config.max_position_embeddings
    out = model.generate(
        inputs.input_ids,
        attention_mask=inputs.attention_mask,
        max_length=max_len,
        pad_token_id=tokenizer.eos_token_id
    )
    gen_ids = [o[len(i):] for i, o in zip(inputs.input_ids, out)]
    resp = tokenizer.batch_decode(gen_ids, skip_special_tokens=True)[0]

    # åŠ å…¥åŠ©æ‰‹
    st.session_state.history.append({"role": "assistant", "content": resp})

# æ¸²æŸ“å¯¹è¯åŒºåŸŸ
st.markdown("<div class='chat-container'>", unsafe_allow_html=True)
for msg in st.session_state.history:
    role = msg["role"]
    text = msg["content"].replace("\n", "<br/>")
    if role == "user":
        st.markdown(f"<div class='user-bubble'>{text}</div>", unsafe_allow_html=True)
    else:
        think, ans = split_text(msg["content"])
        if think:
            # æœ‰æ€è€ƒè¿‡ç¨‹
            st.markdown(f"<div class='assistant-bubble'>{ans.replace('\n','<br/>')}</div>", unsafe_allow_html=True)
            st.markdown(f"<div class='think-block'>ğŸ§  {think.replace('\n','<br/>')}</div>", unsafe_allow_html=True)
        else:
            st.markdown(f"<div class='assistant-bubble'>{text}</div>", unsafe_allow_html=True)
st.markdown("</div>", unsafe_allow_html=True)
